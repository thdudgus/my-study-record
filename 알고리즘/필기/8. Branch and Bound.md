**backtracking = DFS + bound function**
Q. DFS를 BFS visit으로 전환하면? 
A. branch and bound
=> is new algorithm!

Q. Backtracking vs Branch and Bound
A. Backtracking = DFS + bound function
Branch and Bound = DFS + bound function
=> 차이는 DFS vs BFS

Q1. DFS와 BFS의 장단점.
A. in practice, B&B가 backtracking보다 더 선호됨. (BFS >> DFS)
BFS : reliable(optimal을 찾아나가는 데 더 안정적임.)
장단점 정리하기..!

Q2. f = g + h < Best (bound function) 관점에서 비교하면? (0/1 Knapsack)
A. tree traverse(child visit) 순서에 따라 g value, h value의 update 차이가 발생.
- 미래 estimate인 h-value가 좋으면(정확한 예측이 가능하면), 잘 맞춘다. 
=> BFS로 진행하는 것이 좋다. 
==> start root 근처에서 pruning이 일찍 발생할 수 있다.
- backtracking은 탐색을 진행할 수록, g-value가 빨리 갱신된다.
=> g-value가 정확해지고, h-value에 대한 불확실성이 빠른 속도로 줄어든다.
key point : h-value 설계/추정/수식 유도 등의 경험을 통해 자신감이 향상되면, B&B가 우월함. 
	그렇지 않으면 backtracking 선호.
## TSP
== Traveling Salesman Problem
== Traveling Sales-person Problem

the most widely-used combinational optimization problem in CS.

- 택배 시스템. 1000개의 박스들 -> 가장 먼저 퇴근하려면 어떤 경로?

Q. h-value estimation in TSP

**Maximize optimization problem** (ex. 0/1 KS)
: upper bound 계산 in each node
-  Overestimate하면 실수(incorrect pruning)하지 않는다.
-  0 < h* < h < infinite		
	h는 h\*과 가까운 게 좋음.(h\*보다 큰 값 중에서 가장 작은 값.) (higher pruning power) 

**Minimize optimization problem** (ex. TSP)
: lower bound 계산 in each node
-  Unerestimate하면 실수(incorrect pruning)하지 않는다.
- 0 < h < h\* < infinite
	h는 h\*과 가까운 게 좋음.(h\*보다 작은 값 중에서 가장 큰 값.) (higher pruning power) 
 
### example) 5 city TSP (비대칭, fully connected)
![[Pasted image 20240620023459.png|400]]

BF, D&C, DP, Greedy, Backtracking, B&B

BF : O(n!)  --> B&B (f = g + h)
	g = actual edge cost (exact)
	h = ??? = h\* - alpha

h-value : not fixed
(교과서 기준)
1. TSP >= min. outgoing edges of node A
2. Tsp >= min. incoming edges of node B
=> 가장 작은 edge로 나가고, 가장 작은 edge로 들어온다.
==> h < h\*

![[Pasted image 20240620025554.png|300]]
노드 간 연결은 안 되지만 h 최소.
1번 도시 입장에서 = 4+7+4+2+4 = 21
2번 도시 입장에서 (v2) h = min(7, 8, 7) + (v3) min(4, 7, 16) + (v4) min(11, 9, 2) + (v5) min(18, 17, 4) = 7+4+2+4 = 17
3번 도시 입장에서 (v3) h = min(5, 7, 16) + (v2) min(14, 8, 7) + (v4) min(11, 7, 2) + (v5) min(18, 7, 4) = 5+7+2+4 = 18
...
![[Pasted image 20240620030452.png|350]]
3번 도시 입장에서 (v3) h = min(7, 16) + (v4) min(11, 2) + (v5) min(18, 4) = 7+2+4 = 13
![[Pasted image 20240620031253.png||400]]
남아있는 셀들 계산해보기

### Further advanced issues for B&B
h1 : 무조건 제일 작은 outgoing edges로 연결되는 것을 추정.
h2 : h1을 활용하되, 이미 지나온 노드는 배제하여 추정.
h3 : 일반적으로 bound function은 nique하지 않다.
	-> 1) v2로 들어오는 가장 작은 cost = min of 2nd column = 5
		2) v2에서 나가는 가장 작은 cost = min of 2nd row = 7
		=> 평균 = min cost of v2 = [(1)+(2)]/2 = 12/2 = 6

Q. 
1. h2와 h3의 비교 우위를 논하시오.
2. search tree를 그리면서 답하시오.
3. 각 incoming/outgoing edges ==> 1/2 계산 이슈

Q1. branch factor and DFS/BFS
branch factor and DFS/BFS (브랜치가 넓은 경우 A, 좁은 경우 B)
1. A with backtracking and B with B&B
2. B with backtracking and A B&B
둘 중 더 좋은 건?
Q. branch factor는 큰 것이 좋은가? 작은 것이 좋은가?
	== tree visit type과 관계 있다.
	== branch factor는 설계와 관련 있음 (DFS, BFS)
	==> root에서 가까이 어떤 아이템을 먼저 배치할 것인가?

Q2. h-value estimate: h = h\* +/- alpha라고 하는데 구체적인 가이드라인은 없는가?
(인공지능 강의의 주제 중 하나) : relaxation technique
	simplification == simplification : constraint 제거.
	ex. 0/1 KS : h-value 유도. -> (relax) 제약 조건 : sack에 item을 0 또는 1로 담는다. (빈 공간은 남겨둔다.)
	-> item을 나누어 담을 수 있다. -> h-value = fractional KS가 됨. == h\*

Q3. |V| = n이 huge할 때 TSP는?
ex. n = number of cities in US
A. 새로운 design approach가 필요하다.
-> 13주차 학습 주제. 왜 문제가 어렵고 어떻게 접근 하나?
=> NP class problem ==> approximation algorithm
