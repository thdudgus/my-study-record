### Greedy vs Dynamic
왜 greedy algorithm은 중요할까?
1. natural, simple, fast/easy implementation : 경험과 지식에 기반한(heuristic) idea
2. Test-bed : 여러 실험들과 여러 분석에 대한 기준점(시작점) baseline performance 제공. 기준점
- when we face new/open probem : 연구자들이 가장 먼저 접근하는 설계 전략.
- 정밀도/정확도 accuracy 향상, 속도증가, model size 감소
처음 생각한 방식에서 더 발전시킬 근거를 줌.
3. may be 'optimal' : optimal substructure
- local optimal --> global optimal : proof required

# Knapsack problem
constrained combinatorial optimization problem
S = {item1, item2, ..., itemn} // 문제 구성 요소, object
w_i = {weight1, weight2, ..., weightn}
p_i = {profit1, profit2, ..., profitn}
W = sack size;
==> find A (a subset of S) such that 
1) maximizes the profit in A
2) sum of the weights in A <= W

example1. 도둑이 가방(sack)을 가지고 보석방에 감. 보석(무게/부피/가치)
-> 보석 아이템의 일부만 담을 수 있다.(constraint, ..., 제약)
--> 도둑boss의 질문 : optimal? highest prices?

example2. startup company : fund raising.
-> 투자가 : 사업아이템, 전체투자금(constraint. 제한)
-> 아이템 : 사업주제, 필요 자금, 수익 : 일부만 투자할 수 있다
==> constraint optimization 

## Greedy algorithm for 0/1 KS
Q. solve the 0/1 KS.
A.
1. BF: 모든 가능한 솔루션 다 찾기. 2^n enumeration -> EXP time complexity (n: huge)
2. D&C: optimal을 찾을 수 있는가? optimal but inefficient?
3. DP: 
4. Greedy: optimal solution이 발견될 수 없음.
	=> 언제 greedy는 optimal solution을 제공할까?
		= kg당 가치가 높은 걸 택하는 idea2가 0/1 KS가 fractional KS로 변경되면 optimal. 
		0/1은 item이 들어갈 수도, 못 들어갈 수도 있음. 
		fractional은 남은 sack에 item을 나누어서 일부분을 담을 수 있다. (idea2에서 남은 공간 5kg에 item2를 분할해서 담는 것. => optimal)

**Greedy**
idea1) Largest profit first : simple rank
(w1 w2 w3) = (25kg 10kg 10kg)
(p1 p2 p3) = ($10 $9 $9)
W = 30kg
=> A = {item1} 현재 무게 25kg -> $9인 item을 담으면 35kg라 안 됨.
=> optimal A = {item2, item} 총 이익 = $18

idea2) largest profit per unit weight first
(w1 w2 w3) = (5kg 10kg 20kg)
(p1 p2 p3) = ($50 $60 $140)
W = 30kg
=> pi/wi = ($10 $6 $7)
=> A = {item1 item3} 총 profit=50+140
==> optimal A = {i2, i3}

**DP** for 0/1 Knapsack problem - recurrence equation
Q. DP key point - optimal substructure? YES
-> principle of optimality? YES
---> local subproblems -> global optimal sol. (이유 찾아보기)

Design : recurrence equation (recursive prop)
Think yourself. 몇차원 array/table 필요할까?

후보 1 : 1차원 array
item을 선택/담는 상태에서 profit 최대치 -> 기록.
**P[i] = ith_item까지 선택한 상태의 profit 누적.**

P[i] = APSP i번째 도시를 방문/경유할지 결정하는 문제와 유사
P[i] = max 1) item_i를 선택/담는 경우 2) item_i를 못 담는 경우 
	= max {P[i-1] or P[i-1]+p_i} => NO correct(weight 고려가 없음.)

후보 2 : 2차원 array (아이템, 무게)
P[item_i, w] = max item_1를 못 담는 경우 또는 담는 경우 (sack에 포함될 수 있음.)
P[i, w] = max P[i-1, w] or P[i-1, w]+p_i ==> NO correct (item_i를 담기 위해 그 전 sack에 w_i만큼이 비어 있어야 한다. w_i에 대한 정보가 없음)

Final : 현재의 weight를 보고 item_i를 담을지 말지 판단.
P[i, w] = max 1) P[i-1, w] : item_i를 넣을 수 없는 경우 or 
		2) P[i-1, w-w_i]+p_i : item_i를 넣을 수 있는 경우 (현재 있는  w에 w_i가 비워져 있어야 하고, 그 전 weight 상태에서 i-1의 profit이 얼마나 있는지 알고 있어야 함.  

example: 
S = {item1, item2, ..., item4}
W = 10kg (sack size)
w_i, p_i = {weight1, profit1을 가지고 있는지 정보}
p_i = (10 40 30 50)
w_i = (5kg 4kg 6kg 3kg)

P[i, x]
![[Pasted image 20240619172451.png]]
P[0, n], P[n, 0]은 dynamic programming의 축.

![[Pasted image 20240619173044.png|500]]
각 kg이 제시된 상황에서 담을 수 있는지 없는지 판단하면 됨. knapsack의 무게임

item0 이 knapsack의 어떤 무게에 관계 없이 풀 수 있음. 총 profit은 0
knapsack이 0이면 item을 담을 수 없어 총 profit은 0
P[1, 4kg] = P[i-1, w] = P[1-1, 4] = 0.

P[1, 5] = max 1) or 2) = max 2) =  P[i-1, w-w_i]+p_i = P[0, w-5] + p_1 = P[0, 0] +p_1 = 10
w-5는 1번 아이템이 sack에 차지한 kg을 제외한 부분. 직전 아이템이 어떤 max를 리턴하는지.
![[Pasted image 20240619175443.png]]

item_2 고민 == Floyd for APSP (2번 도시 선택/경유 문제는 1번 도시까지의 subproblem이 모두 해결된 상태에서 2번의 이슈를 추가 계산)
=> item1까지의 모든 문제들이 다 해결이 돼있는 상태에서 item2를 계산. item1의 행들이 reuse.
P[2, 4] = max 1) P[1, 4] = 0
		2) P[그전 계산값] + p_2 = P[1, w-w2] + p_2 = P[1, 4-4] + p_2 = 0 + 40 = 40
	= 둘 중 max인 40 
P[2, 5] = max 1) P[1, 5] = 10
		2) P[i-1, w-wi] + p_2 = P[1, 5-4] m+ p_2 = P[1, 1] + p_2 = 0 + 40
		= 40
![[Pasted image 20240619180532.png]]
P[2, 9] = max 1) P[1, 9] = 10
		2) P[1, 9-w_2] + p_2 = P[1, 9-4] + 40 = P[1, 5] + 40 = 50
	둘 중 max인 50
![[Pasted image 20240619180719.png]]
나머지 부분 채워보기
 ![[Pasted image 20240619181011.png]]
 P[4, 10]이 정답이 됨.

APSP, CMM처럼 그 경로(뭘 담아야 하는지)를 알려면, for-loop에서 max1과 2중 뭐가 선택되었는지(index)를 저장하고,복귀하며 그걸 프린트 하면 됨. 

## DP analysis for 0/1 Knapsack: T.C
time complexity T(n, W) = TC for each subproblems * # of subproblems = theta(1) * n\*w = O(n\*w) = theta(n\*w)
하나의 셀을 푸는 수식의 값은 theta(1).
--> 만약 W가 huge (real number) => terrible performance
--> polynomial time complexity : input size인 n의 값에 의해 결정된다는 것.
그러나 w value가 중요. depends on numeric value of input w
	 (NOT of number of inputs)
==> Pseudo polynomial time complexity

===> 이를 해결하기 위해 노력해옴. idea : do not need to compute all elements in P[i, w]
두 셀의 정보만 알면 P[i, w]를 계산할 수 있음. 
=> DP optimization (implementation)
operation = 1 + 2 + 2^2 + ... + 2^(n-1) = O(2^n)

Final TC = min {O(n\*w), O(2^n)}

### Remarks on DP performance for some problems
DPvsBF가 경쟁력이 없음
Q. 왜 이런 현상이 발생했을까? BF, D&C, DP, Greedy
A. 문제의 난이도가 다르기 때문. (Knapsack문제는 매우 어려운 문제이기 때문. >>>  Sort, Search, CMM, SP)

=> we need a new approach

## Backtracking: 0/1 Knapsack
(backtracking page 본 후에 보기.)

w_i = (w1, w2, ..., wn)
p_i = (p_1, p_2, ..., p_n)
W = knapsack size
=> find a subset of items A

**promising function**
1. weight > W(fixed)
2. (subset sum) : weight + w(next item) > W(fixed) or weight + w(total sum of next items) < W(fixed) 
위 조건들 중 하나라도 해당되면 stop, not expand, nonpromising
그런데 0/1 KS에선 기준점이 변한다. 왜? -> best profit을 고려해야 하기 때문.
=> Nonpromising function == bounding function

멈추다 == bound

**bound function**
= 지금까지의 profit(g) + 남은 아이템 profit의 최대 추정치(예측, h)
```
bound function = g + h < BEST (nonpromising)일 때 멈춘다. 
```
h : 사람마다 다르게 계산/제시할 수 있다.

bound function 0/1 Knapsack problem
```
bound function = g(현재 profit) + h(미래예측 : idea 승부처) < Best

g = (item1, item2, ..., itemi)담았을 때 sum of p_j (j=1, 2, ..., i)
h = (itemi+1, itemi+2, ..., itemk)담았을 때 sum of p_j (j=i+1, ..., k) 
	+ 
	(itemk+1의 일부분) (p+k+1/w_k+1) * (W-(w1+w2+...+wk))
```
heuristic : 경험, insight

Q. estimator h는 큰 값을 예측하도록 설계하는 것이 좋은가?
-> 전체적인 pruning power가 증가되는가?
-> 한편으로는, incorrect prunning이 발생하지 않는가?

example
p = (40 30 50 10)
w= (2kg 5kg 10kg 5kg)
W = 16kg

backtracking으로 해결. DFS visit drawing으로 해결.
단, h-함수는 제시된 것을 활용한다.

아래 그림의 노드 정보
- 담았을 때(현재) p
- weight
- 예측되는 g+h
![[Pasted image 20240620002615.png]]


## Branch and Bound: 0/1 Knapsack
BFS 구현할 땐 queue 사용.
![[Pasted image 20240620012842.png]]
그래프가 커질 수록 효율적. 

### Improve the standard Branch and Bound
- idea : based on the visit order of children nodes
- BFS, DFS : fixed visit order (방문 traverse 순서)
=> 알고리즘이 수행되는 동안 방문 순서를 바꿔보자.

**new B&B algorithm**
== Branch and Bound with best-first (search)
== best-first branch and bound
== (인공지능) A * algorithm
![[Pasted image 20240620013958.png]]
BFS + DFS와 같은 느낌

Q.  : Best-first B&B는 기본적으로 greedy approach를 따라감. (local optima choice) 이 경우 생각해봐야 할 점은?
A. 이것이 optimal한지?
기존의 방법은 all enumerations 가능.
proof 필요. 

![[Pasted image 20240620014054.png]]
위 경우 직접 그려보깅~

### H-value (학부 이후)
어떻게 h-value를 estimate하나?
h : mathematical expression for estimation/prediction
- 0/1 Knapsack problem : h = fractional KS.

**guideline for h=value**
: h=value의 range를 설정하며 범위를 좁혀감.
**Maximize optimization problem** (ex. 0/1 KS)
1. h > 0, h < infinite ==> 0 < h < infinite
2. h * : global optimal value  ==> h와 h\*의 관계성 유추
		1) h > h* : overestimate
		2) h< h* : underestimate
3. Overestimate하면 실수(incorrect pruning)하지 않는다.
		1) optimal로 진행 가능한 가지를 pruning out하지 않음. 
		2) 보수적, 안정적인 pruning
		3) pruning되는 subtree 가지는 적을 수 있음. (ex. 0/1 KS fractional)
4. 0 < h* < h < infinite		
	h는 h\*과 가까운 게 좋음.(h\*보다 큰 값 중에서 가장 작은 값.) (higher pruning power) 
5. Q. 0/1 KS에서 활용한 fractional. KS을 이용한 h-value보다 더 pruning이 많이 되는 h-value 수식을 제시하시오.

0/1 Knapsack problem에선 h가 overestimate expression이 좋음.
=> upper bound ==> Maximization optimization의 형태이기 때문.

Minimization optimization problem(ex. TSP)에선 h가 underestimate value가 좋음. 
=> lower  bound