## ML learning problems.
	보유한 학습(training) data를 기반으로 접근 방법 결정

- data마다 **discrete** label이 존재
	[[Classification]] - 분류 (supervised learning - 교사학습)
	
- data마다 **continuous** label이 존재 
	[[Regression]] -  회귀 (supervised learning - 교사학습)
	
- data마다 label이 없음
	[[Clustering]] - 군집화 (unsupervised learning - 비교사학습, 임베딩, [[Dimensionally Reduction]])

## Training(Build) and Testing(Operation)
![[Pasted image 20240510124853.png|500]]
- chatGPT에서 feature vector를 제공하기 때문에,
- 우리는 적절한 estimation 알고리즘을 선택 및 학습시켜야 한다.

## 알고리즘
- Supervised setting
	- Linear model
	- Nearest Neighbors
	- Support Vector Machines (SVM)
	- Stochastic Gradient Descent
	- Decision Trees
	- Ensembles
	- Neural network models (supervised)
	- ...

- Unsuperviesed setting
	- Clustering
	- Decomposing signals in components (matrix factorization problems)


## Supporting Technique
- Cross Validation (교차 검증) : 데이터가 적을 수록 유용하게 쓰임.
	교차 검증을 통해 모델의 견고성 향상.
	모델이 전체 데이터 포인트를 포함하기 때문에 모든 가능성을 평균화하여 평균 오차를 제공.
	![[Pasted image 20240510145409.png|450]]
	![[Pasted image 20240510145635.png|500]]
	

## Variance / Bias, [[overfit]] / [[Underfit]]
데이터의 개수나 특성에 맞게 \~~되었나 판단할 근거.

parameter가 너무 적은 model은 **==large bias==**, 부정확.**(==not enough flexibility==)**
	**==Underfit==**
	![[Pasted image 20240510150258.png|400]]

parameter가 너무 많은 model은 ==**large variance**==, (==**표본(sample)에 대한 sensitivility가 너무 높음**==) 부정확.
	**==Overfit==**, noise까지 fitting
	![[Pasted image 20240510150311.png|400]]

### 예시 : kNN
k-Nearest Neighbor Classification
각 test data point에 가장 가까운 training data label을 할당.
![[Pasted image 20240510151130.png]]
![[Pasted image 20240510151208.png]]
**k가 너무 작아 변화에 민감 : Overfit**
**k가 너무 커 변화에 둔감 : Underfit**

![[Pasted image 20240510152446.png]]



## Underfitting and Overfitting
- **Underfitting** : model이 너무 simple 하거나 변화에 둔감. to represent all the relevant class characteristics.
	높은 bias
	(usually) 낮은 variance
	높은 training error
	높은 test error
	=> bias가 높으니까 대체로 variance가 낮아지고, 변화에 둔감하기 때문에 training data가 높고, test error도 높음.
	variance(분산)가 작아서 모여있어 underfit, 변화에 둔감.

- **Overfitting** : model이 너무 complex. 변화에 민감. fits irrelevant characteristics(noise) in the data.
	낮은 bias
	(usually) 높은 variance
	낮은 training error
	높은 test error
	=> bias가 낮으니까 대체로 variance가 높아지고, 변화에 민감하기 때문에 training data는 낮지만, test error는 높음. 
	variance(분산)가 크니까 넓게 퍼져있어 overfit, 변화에 민감

underfit, overfit 둘다 test error는 높아서 피해야 함. 그러나 overfit은 넓게 퍼져 있어 training error는 낮음.

![[Pasted image 20240510152207.png]]
![[Pasted image 20240510152246.png]]
variace와 bias는 trade-off 관계.

testing(validation) loss를 보며, 빠른 stop을 해야할 필요가 있음.
![[Pasted image 20240510152343.png|300]] ![[Pasted image 20240510152405.png|300]]

