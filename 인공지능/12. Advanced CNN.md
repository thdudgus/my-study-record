**Lec 12**
![[Pasted image 20241125003300.png]]
#### **Characteristics 1**
- **이미지는 임의의 벡터가 아니라 공간적 구조(spatial structure)를 가진다**  
    이미지는 단순한 데이터 벡터로 나타낼 수 있는 것이 아니라, 픽셀 간의 위치 관계와 구조가 중요한 특성을 가진다는 것을 의미합니다. 예를 들어, 이미지의 픽셀 배열이 바뀌면 같은 데이터를 표현하지 않을 수 있습니다.
- **이미지의 내용은 크기나 변환이 바뀌어도 유사할 수 있다**  
    이미지는 크기가 조정되거나 위치가 이동하거나, 회전 등의 변환이 이루어져도 본질적인 정보(예: 개의 사진)는 유사하게 유지될 수 있습니다.

#### **Characteristics 2**
- **이미지는 계층적(hierarchical) 특성을 가진다**  
    이미지는 단순한 선(line)으로부터 복잡한 형태(shape), 그리고 더 나아가 패턴(pattern)으로 확장되는 계층적인 구조를 가지고 있습니다.
    - **예시**
        - 선(Line): 단순한 픽셀의 집합
        - 형태(Shape): 여러 선들이 결합해 모양을 형성
        - 패턴(Pattern): 여러 형태들이 조합되어 더 복잡한 이미지 형성
- **이미지 예시**
    - 첫 번째 그림(숫자 7): 이미지는 회전되거나 위치가 달라져도 숫자 "7"의 정체성은 유지됩니다.
    - 두 번째 그림(강아지): 강아지 사진은 회전이나 이동에도 여전히 같은 사진임을 알 수 있습니다.
    - 세 번째 그림(얼굴): 얼굴 이미지를 통해 작은 특징(눈, 입)들이 결합해 전체적인 얼굴 형태를 형성합니다.

"우리는 이미지를 **위치, 회전, 크기(scale)** 변화에 무관하게(hierarchical 특성을 유지하며) 어떻게 특성을 추출할 수 있을까?"  
=> 이는 이미지 처리 및 컴퓨터 비전의 중요한 연구 주제입니다. 예를 들어, CNN(Convolutional Neural Network)과 같은 딥러닝 모델은 이러한 특징을 추출하는 데 효과적입니다.

---
![[Pasted image 20241125003450.png]]
#### **Fully-Connected Layer의 의미**
- **Fully-Connected Layer란?**
    - 이미지의 **모든 픽셀을 한 번에 처리**하여 입력으로 사용하는 방식입니다.
    - 모든 픽셀이 동일한 중요도를 가지고 하나의 벡터로 입력됩니다.
- **Fully-Connected Layer의 문제점**
    - **많은 가중치 파라미터가 필요**:  
        이미지의 크기가 크다면, 모든 픽셀마다 가중치를 학습해야 하므로 파라미터의 수가 급격히 증가합니다. 이는 계산 복잡도와 메모리 요구 사항을 크게 증가시키는 요인이 됩니다.

#### **이미지 처리의 비효율성**
- **비효율성의 원인**
    - 이미지의 공간적 구조(spatial structure)를 무시하고 단순히 픽셀 값을 하나의 벡터로 변환하여 처리합니다.
    - 모든 픽셀 간의 관계를 고려하므로 불필요하게 많은 연산이 요구됩니다.
- **비교 그림**
    - **왼쪽 그림**: MLP를 사용하는 경우, 모든 픽셀을 고려한 완전 연결 방식으로 이미지를 처리합니다. 이는 구조 정보를 놓칠 수 있습니다.
    - **오른쪽 그림**: 보다 효율적인 대안(예: CNN)을 암시하며, 이미지를 작은 부분(패치)으로 나누어 처리하는 방식을 나타냅니다. 이렇게 하면 지역적 특징을 효과적으로 학습할 수 있습니다.

**"Can we process images in a more efficient and effective way?"**
    - 이 질문은 MLP 기반 방식의 비효율성을 극복하기 위해 더 나은 접근 방식을 찾는 것이 핵심임을 강조합니다.
    - 해결 방법의 예로 **Convolutional Neural Networks(CNN)** 이 자주 언급됩니다. CNN은 이미지의 공간적 구조를 활용하고, 가중치 공유(weight sharing) 기법으로 파라미터 수를 줄이는 데 효과적입니다.

---
![[Pasted image 20241125004137.png]]
#### **Convolutional Neural Networks (CNN)란?**
- **정의**  
    CNN은 **구조화된 그리드 형태의 데이터(예: 이미지)** 를 처리하도록 설계된 딥러닝 모델의 한 종류입니다.
    - 이미지는 픽셀의 2차원 배열(그리드)로 표현되므로, CNN은 이러한 데이터를 효과적으로 분석하는 데 적합합니다.
#### **CNN의 구성 요소**
CNN은 크게 두 부분으로 나눌 수 있습니다.
##### **(1) 특징 추출 (Feature Extraction)**
- **Convolution Layers (합성곱 층)**
    - 이미지를 작은 필터(커널)로 스캔하여 지역적 특징(에지, 모서리 등)을 추출합니다.
    - 각 필터는 특정한 특징(예: 수평선, 곡선)을 학습합니다.
- **Pooling Layers (풀링 층)**
    - Convolution Layers에서 추출된 특징 맵(feature map)의 크기를 줄이기 위해 사용됩니다.
    - 일반적으로 Max Pooling(최대값을 선택) 또는 Average Pooling(평균값을 선택)을 사용합니다.
    - 데이터 크기를 줄여 계산 효율성을 높이고, 과적합(overfitting)을 방지합니다.
##### **(2) 의사 결정 (Decision Making)**
- **Fully-Connected Layers (완전 연결층)**
    - Convolution과 Pooling Layers에서 추출된 특징을 Flatten(1차원 벡터로 변환)하여 최종적으로 클래스를 분류합니다.
    - 입력 데이터를 특정 클래스(예: 컴퓨터, 자동차, 개 등)로 매핑합니다.

#### **시각적 설명**
- **왼쪽부터 오른쪽까지 과정**
    1. **입력(Input)**: 원본 이미지(예: 컴퓨터)를 입력으로 받습니다.
    2. **Convolution & Pooling**:
        - Convolution Layers에서 이미지를 필터링하여 특징을 추출하고,
        - Pooling Layers에서 데이터 크기를 줄이며 중요한 정보를 유지합니다.
    3. **Flatten 및 Fully-Connected Layers**: 특징을 1차원으로 변환한 후, Fully-Connected Layer를 통해 클래스 확률을 계산합니다.
    4. **출력(Output)**: 최종적으로 해당 이미지가 특정 클래스(컴퓨터, 자동차, 개 등)에 속할 확률 값을 출력합니다.

#### **결론**
이 슬라이드는 CNN이 이미지를 처리하는 데 있어 **효율적이고 효과적인 구조**를 제공함을 강조합니다.  
CNN은 **특징 추출**과 **분류**라는 두 단계로 구성되며, 이를 통해 이미지 데이터의 복잡한 패턴을 학습하고 분류 작업에 활용합니다.

---
![[Pasted image 20241125004403.png]]
이 슬라이드는 **Convolution Layer(합성곱 층)**의 동작 과정을 설명합니다.
#### Convolution 과정
**Convolution Layer** 는 입력 이미지와 필터(커널)를 이용해 특징 맵(feature map)을 생성합니다.  
다음은 합성곱의 주요 단계입니다:
1. **필터를 입력 행렬 위에 놓기**
    - 필터(Trainable parameters)를 입력 이미지의 특정 위치에 겹칩니다.
2. **원소별(element-wise) 곱셈 수행**
    - 필터의 각 값과 입력 행렬의 해당 위치 값들을 곱합니다.
3. **모든 값을 더하기 (Summation)**
    - 곱셈 결과를 모두 더해 하나의 값으로 만듭니다.
    - 이 값은 출력 행렬(output matrix)의 해당 위치에 저장됩니다.

#### **예제 설명**
- **입력 행렬(input matrix):**  
    4×4 크기의 행렬이 입력으로 주어집니다.
- **필터(filter):**  
    3×3 크기의 필터(2, 0, 1; 0, 1, 2; 1, 0, 2)가 사용됩니다.
- **계산 과정:**
    - 첫 번째 위치에서 계산된 값 : (1×2)+(2×0)+(3×1)+(0×0)+(1×1)+(2×2)+(3×1)+(0×0)+(1×2)=15
    - 두 번째 위치에서 계산된 값 : (2×2)+(3×0)+(0×1)+(1×0)+(2×1)+(3×2)+(0×1)+(1×0)+(2×2)=16
- **출력 행렬(output matrix):**  
    입력 행렬과 필터를 합성곱한 결과로 출력 행렬이 생성됩니다.

#### **출력 행렬의 의미**
- 출력 행렬의 각 셀은 입력 행렬의 특정 영역에서 필터를 적용해 계산된 값입니다.
- 이 값은 입력 이미지의 특징을 나타내며, CNN의 특징 추출 과정에서 중요한 역할을 합니다.

**"What is the meaning of each cell in the output matrix?"**
- **출력 행렬의 각 셀**은 **입력 행렬과 필터 간의 합성곱 결과**로, 입력 이미지의 특정 부분(특징)이 얼마나 강조되었는지를 나타냅니다.
- 이는 필터가 특정 패턴(예: 가장자리, 텍스처)을 얼마나 잘 감지했는지 보여줍니다.

---
![[Pasted image 20241125005439.png]]
#### **출력 크기 계산 공식**
출력 크기를 계산하는 공식은 다음과 같습니다:
- W: 입력의 크기 (너비 또는 높이)
- K: 커널 크기 (필터 크기)
- P: 패딩 크기
- S: 스트라이드 크기
- W′: 출력 크기

##### **슬라이드 예시**
###### **조건**
- 입력 크기: 32×32×3 (너비, 높이, 채널)
- 커널 크기: 5×5
- 패딩: P=0
- 스트라이드: S=1
- 필터 개수: F=4
따라서, 출력 너비와 높이는 28×28
**출력 채널:**
- 필터 개수(F)가 출력 채널(C)이 됩니다.
- 여기서는 C′ =F = 4
###### **최종 출력**
- 출력 크기: 28×28×4

#### **각 변수의 역할**
- **커널 크기 (K)**
    - 큰 커널은 더 많은 픽셀을 한 번에 처리하지만, 출력 크기를 줄입니다.
- **패딩 (P)**
    - 입력 데이터의 크기를 유지하거나 출력 크기를 조정하는 데 사용됩니다.
- **스트라이드 (S)**
    - 스트라이드가 크면 필터가 더 많이 이동하므로 출력 크기가 작아집니다.
- **필터 개수 (F)**
    - 출력 채널의 수를 결정합니다. 필터가 많을수록 더 다양한 특징을 학습할 수 있습니다.

#### **요약**
- Convolution Layer에서 **출력 크기는 커널, 패딩, 스트라이드, 필터 개수와 같은 매개변수에 의해 결정**됩니다.
- 이 공식은 CNN 구조 설계 시 출력 크기를 미리 계산하여 네트워크를 최적화하는 데 매우 유용합니다.

---
![[Pasted image 20241125005809.png]]
![[Pasted image 20241125005835.png]]
##### **추가 계산: 바이어스**
- 각 출력 채널마다 하나의 바이어스 파라미터가 추가로 필요합니다.
- 여기서는 Cout = 4이므로, 바이어스 파라미터는 **4개**입니다.
- 총 파라미터 수:
    Total parameters=Number of weights+Biases=300+4=304

---
![[Pasted image 20241125010249.png]]
#### **AlexNet의 주요 특징 (Key Features of Architecture)**
AlexNet은 2012년 ILSVRC에서 딥러닝을 사용해 큰 성공을 거둔 최초의 모델입니다. 주요 특징은 다음과 같습니다:
1. **GPU 기반 구현 (GPU Implementation)**
    - GPU를 사용해 대규모 데이터와 딥러닝 모델의 학습을 가능하게 했습니다.
    - 이로 인해 연산 속도가 크게 향상되었습니다.
2. **ReLU 활성화 함수의 최초 사용 (First use of ReLU)**
    - 이전의 Sigmoid나 Tanh 함수 대신 ReLU(Rectified Linear Unit)를 사용했습니다.
    - 학습 속도를 크게 향상시켰으며, 기울기 소실(Vanishing Gradient) 문제를 줄였습니다.
3. **정규화 층 (Normalization Layer)**
    - 로컬 응답 정규화(Local Response Normalization, LRN)를 도입하여 과적합을 방지하고 일반화 성능을 향상시켰습니다.
4. **데이터 증강 (Heavy Data Augmentation)**
    - 이미지 회전, 자르기, 반전 등을 활용해 데이터를 증강시켰습니다.
    - 이는 모델의 일반화 능력을 높이는 데 기여했습니다.
5. **5개의 합성곱 층과 3개의 완전 연결 층 (5 Convolutional Layers + 3 Fully-connected Layers)**
    - 초기 단계에서 저수준 특징을 학습하고, 마지막 단계에서 고수준 특징을 사용해 분류를 수행합니다.

#### **하이퍼파라미터 (Hyperparameters)**
AlexNet의 학습 설정은 다음과 같습니다:
1. **Dropout (p=0.5)**
    - Fully-connected Layer에서 50%의 **노드를 랜덤으로 비활성화하여 과적합을 방지**했습니다.
2. **Batch Size = 128**
    - 한 번의 학습 단계에서 사용하는 데이터 배치 크기입니다.
3. **SGD Momentum = 0.9**
    - Stochastic Gradient Descent(SGD)에 모멘텀을 추가하여 최적화 속도를 향상시켰습니다.
4. **Learning Rate = 0.01 (1e-2)**
    - 학습 속도를 제어하는 초기 학습률입니다.

#### **네트워크 구조 (오른쪽 그림)**
- **Convolutional Layer:**
    - 이미지의 공간적 특징을 추출하는 합성곱 층입니다.
- **Fully Connected Layer:**
    - 학습된 특징을 사용하여 분류를 수행하는 층입니다.
- 네트워크는 계층적으로 구성되어 있으며, 각 층은 입력 데이터를 점진적으로 압축하고 학습합니다.

#### **AlexNet의 의의**
- **딥러닝 대중화의 시작**
    - AlexNet은 GPU를 활용한 딥러닝의 성공 사례로, 이후 딥러닝 연구와 산업적 활용을 촉진시켰습니다.
- **컴퓨터 비전의 혁신**
    - ILSVRC에서 AlexNet의 성공은 CNN(Convolutional Neural Network)이 이미지 처리 및 분석에 매우 강력하다는 것을 증명했습니다.

![[Pasted image 20241125010503.png]]
![[Pasted image 20241125010520.png]]![[Pasted image 20241125010535.png]]

---
![[Pasted image 20241125010606.png]]
#### **VGGNet의 핵심 아이디어**
- **"Toward Deeper Convolutional Neural Networks"**  
    VGGNet은 **깊은 네트워크 구조**를 통해 더 복잡한 특징을 학습하려는 접근법을 보여줍니다.
- **작은 필터(3x3) 사용**
    - 3x3 크기의 필터를 여러 층으로 쌓아 더 큰 필터(예: 7x7)와 동일한 수용 영역(receptive field)을 만듭니다.
    - 단일 7x7 필터 대신 세 개의 3x3 필터를 사용해 계산 효율성을 높이고, 학습 가능 파라미터 수를 줄이면서 더 많은 비선형성을 도입합니다.

#### **작은 필터의 이점**
1. **동일한 수용 영역**
    - 세 개의 3x3 합성곱 층(Stride=1)을 사용하면 단일 7x7 합성곱 층과 동일한 수용 영역(7x7)을 가집니다.
    - **입력 데이터를 점진적으로 처리하면서 정보를 더 세밀하게 학습**합니다.
2. **더 적은 파라미터 수**
    - 단일 7x7 필터의 파라미터 수: 7×7×Cin​×Cout​
    - 세 개의 3x3 필터의 파라미터 수: 3×(3×3×Cin​×Cout​)
    - 작은 필터를 사용하면 학습해야 할 파라미터 수가 크게 줄어들어 과적합(overfitting)을 방지합니다.
3. **더 많은 비선형성**
    - 각 3x3 필터 뒤에 활성화 함수(ReLU 등)가 추가되며, 비선형성을 도입해 모델의 표현력을 향상시킵니다.
4. **효율적인 특징 추출**
    - 큰 필터는 단일 단계에서 정보를 학습하지만, 작은 필터는 여러 단계를 통해 더 정교한 특징을 학습합니다.

##### **3. 그림 설명**
- **Conv1(3x3)**: 입력 데이터에서 작은 영역의 특징을 학습.
- **Conv2(3x3)**: 이전 층의 출력으로부터 더 복잡한 패턴 학습.
- **Conv3(3x3)**: 더 높은 수준의 특징을 학습.
- **결론적으로**: Conv1+Conv2+Conv3의 수용 영역은 7×77 \times 77×7과 동일하지만, 더 세밀하고 효율적인 학습이 이루어집니다.

#### **VGGNet의 설계 원칙**
- 단순하고 반복적인 네트워크 디자인:
    - 모든 합성곱 층에 동일한 크기의 작은 필터(3x3)를 사용.
    - Pooling 층을 추가해 점진적으로 크기를 줄이며 학습.

#### **결론**

- VGGNet은 깊은 네트워크 설계에서 작은 필터를 반복적으로 사용하는 접근법을 제안했으며, 이는 계산 효율성과 성능 간의 균형을 유지하면서 더 높은 수준의 특징 학습을 가능하게 합니다.
- 이는 이후 딥러닝 모델 설계에 표준적인 방법론으로 자리 잡았습니다.

---
![[Pasted image 20241125010810.png]]
##### **문제 제기: 단순히 층을 깊게 하면 어떻게 될까?**
- **그래프 설명**:
    - **Train Error (훈련 오차)**:
        - 20-layer 모델은 훈련 오차가 낮고 안정적으로 감소.
        - 56-layer 모델은 오히려 훈련 오차가 높은 상태를 유지.
    - **Test Error (테스트 오차)**:
        - 20-layer 모델이 더 낮은 테스트 오차를 보임.
        - 56-layer 모델은 테스트 오차가 높은 상태로 수렴.
    - 결론적으로, **56-layer 모델이 20-layer 모델보다 훈련과 테스트 모두에서 성능이 저하**됩니다.

#### **문제 원인 분석**
##### **Overfitting인가?**
- 일반적으로 깊은 모델은 과적합(overfitting)될 가능성이 높지만, 여기서는 **훈련 오차도 높은 상태**를 유지.
- 따라서 **overfitting이 아닌 다른 문제**임을 시사.
##### **Optimization 문제**
- 깊은 신경망은 층이 많아질수록 최적화가 어려워지는 경향이 있습니다.
    - **Gradient Vanishing (기울기 소실):** 층이 깊어질수록 초기 층으로 전달되는 기울기가 소실.
    - **Gradient Exploding (기울기 폭발):** 반대로 기울기가 비정상적으로 커질 수도 있음.
    - 이러한 문제로 인해 깊은 모델이 효과적으로 학습되지 않음.

#### **결론 및 해결책**
##### **깊은 모델의 어려움**
- 깊은 신경망은 더 많은 비선형성과 학습 능력을 제공하지만, **최적화 문제**로 인해 성능이 저하될 수 있습니다.

#### **해결책**
1. **Residual Connection (잔여 연결)**:
    - ResNet에서 도입된 방식으로, 입력을 출력으로 바로 연결하여 **기울기 소실 문제를 완화**.
    - 네트워크의 최적화를 더 용이하게 만듦.
2. **Batch Normalization**:
    - 각 층의 입력 분포를 정규화하여 학습 안정성 향상.
3. **좋은 초기화 전략**:
    - 적절한 가중치 초기화로 학습을 원활히 함.

#### **결론**
- 단순히 층을 쌓는 방식은 효과적이지 않으며, **깊이를 효율적으로 활용할 수 있는 최적화 전략**이 필요합니다.
- 이 연구는 **ResNet의 등장 배경**을 이해하는 데 중요한 사례를 제공합니다. ResNet은 깊은 모델의 최적화 문제를 해결하며, 깊은 신경망 설계의 혁신을 이루었습니다.

---
![[Pasted image 20241125011003.png]]
#### **Key Idea: Residual Connection**
- Residual Connection은 기존 신경망에 **Identity Map**을 추가하는 구조입니다.
- 일반적인 네트워크(Plain Layers)는 H(X)를 직접 학습하려고 하지만, ResNet은 **잔여 매핑(Residual Mapping)** 인 f(X)=H(X)−X를 학습하도록 설계되었습니다.
- 이는 복잡한 H(X)를 학습하는 대신 더 간단한 f(X)를 학습함으로써 **학습의 효율성과 안정성**을 높입니다.

#### **그림 설명**
##### **Plain Layers**
- 왼쪽 그림:
    - 기존의 합성곱 신경망(Conv + ReLU)은 입력 X로부터 출력 H(X)를 직접 학습.
    - 깊은 네트워크에서는 **Gradient Vanishing/Exploding(기울기 소실/폭발)** 문제가 발생할 가능성이 큼.
##### **Residual Block**
- 오른쪽 그림:
    - Residual Connection을 추가하여 입력 X를 출력으로 직접 전달.
    - 출력은 H(X)=f(X)+X 형태로, Residual Mapping f(X)만 학습.
    - Residual Mapping은 상대적으로 더 간단한 학습 문제를 제공하여 최적화를 용이하게 함.

#### **Residual Connection의 효과**
1. **Gradient Flow 개선**
    - 입력 X가 출력까지 직접 전달되므로, **기울기 소실 문제를 완화**.
    - 역전파 시 경로가 단축되어 안정적인 학습 가능.
2. **더 깊은 네트워크 가능**
    - 잔여 매핑을 학습함으로써 100층 이상의 매우 깊은 네트워크 설계 가능.
    - ResNet은 152층의 네트워크에서도 높은 성능을 보임.
3. **효율적인 학습**
    - H(X)를 직접 학습하는 것보다 f(X)를 학습하는 것이 단순하며, 학습 속도와 성능을 모두 개선.

#### **4. ResNet의 핵심 철학**
- 네트워크가 직접 입력-출력 매핑을 학습하는 대신, 이를 단순화하여 **잔여 정보를 학습**하도록 만듦.
- 이는 최적화 문제를 단순화하고, 딥러닝 모델이 더욱 깊어지면서도 성능 저하 없이 학습할 수 있도록 함.

![[Pasted image 20241125123844.png]]
#### **Key Idea**
##### **Identity Map (Skip Connection 또는 Residual Connection)**
- Residual Connection은 입력 데이터 X를 출력으로 직접 전달하며, 네트워크가 **Residual Mapping f(X)=H(X)−X**을 학습하도록 돕습니다.
- 이를 통해 깊은 네트워크에서 발생할 수 있는 **기울기 소실(Gradient Vanishing)** 문제를 완화합니다.
##### **Stacking Residual Blocks**
- ResNet은 **Residual Block**을 여러 개 쌓아 구성됩니다.
- 각 Residual Block은 **두 개의 3x3 합성곱 층(Convolution Layers)** 으로 구성되며, 이 두 층의 출력에 입력 데이터를 더하는 방식으로 작동합니다.

#### **그림 설명**
#### **Residual Block (왼쪽)**
- 입력 X는 두 개의 합성곱 층(conv + relu)을 통과하여 f(X)를 생성합니다.
- 출력은 H(X)=f(X)+X로 계산됩니다.
- 이 과정은 직접 H(X)를 학습하는 것보다 단순하고 효율적입니다.
##### **전체 ResNet 구조 (오른쪽)**
- 입력 데이터에서 시작하여 **Residual Blocks**와 **Pooling**을 반복적으로 사용하여 특징을 추출.
- 마지막으로 **Fully Connected Layer**와 **Softmax**를 통해 분류를 수행합니다.
- 다양한 색상으로 표시된 블록은 다른 출력 채널 크기(예: 64, 128, 256, 512)를 나타냅니다.

#### **Residual Mapping의 이점**
1. **효율적인 학습**
    - 네트워크는 복잡한 H(X) 대신 단순한 f(X)를 학습하여 더 빠르게 수렴.
2. **기울기 흐름 개선**
    - 입력 X가 출력까지 직접 전달되므로, 깊은 네트워크에서 **기울기 소실 문제**가 감소.
3. **깊은 네트워크 구현 가능**
    - ResNet은 최대 1000개 이상의 층을 가진 네트워크에서도 효과적으로 학습 가능.

#### **ResNet의 설계 철학**
- 네트워크는 원하는 출력 H(X)를 직접 학습하지 않고, 더 단순한 f(X)를 학습하도록 설계.
- 이 방식은 깊은 네트워크에서도 안정적인 학습을 보장하며, 딥러닝 모델의 깊이를 크게 확장할 수 있도록 지원.

![[Pasted image 20241125124126.png]]
#### **실험 목적**
- **Plain 네트워크(잔여 연결 없는 일반 CNN)** 와 **ResNet**의 성능 비교.
- 층 수가 18과 34인 네트워크에서 훈련과 테스트 오차의 차이를 관찰.

#### **그래프 설명**
##### **왼쪽: Plain 네트워크 (잔여 연결 없음)**
- 18-layer와 34-layer Plain 네트워크의 훈련 결과:
    - 층이 얕은 18-layer 네트워크는 훈련 오차와 테스트 오차 모두 낮은 값을 보임.
    - 층이 깊은 34-layer 네트워크는 오히려 훈련 오차와 테스트 오차가 더 높음.
    - 결론: 단순히 층을 깊게 쌓으면 오히려 성능이 저하될 수 있음.
##### **오른쪽: ResNet (잔여 연결 포함)**
- 18-layer와 34-layer ResNet의 훈련 결과:
    - ResNet은 층이 깊어질수록 성능이 개선.
    - 34-layer ResNet이 18-layer ResNet보다 낮은 훈련 오차와 테스트 오차를 기록.
    - 결론: Residual Connection은 네트워크가 깊어지더라도 학습 성능을 안정적으로 유지하고 개선할 수 있게 해줌.

#### **중요한 관찰**
- **Plain 네트워크**
    - 층이 깊어질수록 최적화 문제가 발생하여 성능 저하.
    - 이는 **기울기 소실(Gradient Vanishing)** 과 같은 문제 때문.
- **ResNet**
    - Residual Connection은 네트워크 깊이에 관계없이 최적화를 도와 학습 성능을 향상.
    - 층이 많아질수록 성능이 더 좋아짐.

#### **결론**
- **Residual Connection의 효과**:
    - 네트워크의 깊이에 따른 성능 저하 문제를 해결.
    - 깊은 네트워크에서도 안정적인 학습 가능.
- **ResNet의 우수성**:
    - 실험 결과, ResNet은 동일한 깊이의 Plain 네트워크보다 성능이 뛰어남.
    - 이는 ResNet이 매우 깊은 신경망에서도 **효율적인 최적화**와 **우수한 일반화 성능**을 제공함을 입증.

#### **5. 요약**
- **Plain 네트워크의 한계**:
    - 깊은 네트워크에서 학습이 어려움.
- **ResNet의 장점**:
    - Residual Connection을 통해 깊은 네트워크의 최적화 문제를 해결.
    - 층이 깊어질수록 성능이 더 개선되며, 네트워크 설계에 혁신을 가져옴.


