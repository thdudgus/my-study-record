인공신경망에서 사용되는 [활성화 함수]의 일종

`f(x) = max(0, x)`
입력값이 0보다 크면 그 값을 그대로 출력하고, 0 이하면 0을 출력한다. 
이 단순한 비선형성 때문에, ReLU는 신경망의 학습 속도를 개선하고, 이전에 많이 사용되던 [시그모이드 함수]나 [하이퍼볼릭 탄젠트 함수]보다 효율적으로 깊은 네트워크를 학습시킬 수 있다.

ReLU의 주된 장점은 계산 효율성과 비선형적 특성 때문에 더 복잡한 함수를 모델링할 수 있다는 것이다. 그러나 ReLU는 '죽은 뉴런' 문제를 가지고 있는데, 이는 네트워크의 일부 뉴런이 학습 과정에서 더 이상 활성화되지 않는 현상을 말한다. 이를 해결하기 위해 [Leaky ReLU], [Parametric ReLU] 등의 변형된 ReLU 함수들이 개발되었다.