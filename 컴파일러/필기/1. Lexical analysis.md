1. source program의 input character를 읽는다. 
2. character를 [[lexeme]]이라 부르는 meaningful sequence로 묶는다.   
3. [[token]]의 sequence를 만든다.   
4. token 정보를 symbol table로 저장한다.   
5. token들을 syntax anlayzer로 보낸다.   
![[Pasted image 20240408054018.png]]
<br>

### examle
input string을 substring([[lexeme]])으로 나누고 각 [[lexeme]]의 [[token]]을 식별한다.      
![[Pasted image 20240408060117.png]]   

<br>
### token pattern을 어떻게 specify하나?   
**[[Regular language]]**   
token의 pattern을 describe하기에 powerful, simple   
- [[Keyword]]
- [[Operators]] 중 Comparison
- [[Whitespace]]
- [[Integer]]
- [[Float]]
- [[Identifiers]]
- ...


### input stream으로부터 token을 어떻게 인식하나?  
1. regular expression의 token들을 merge
	Merged = Keyword | Identifier | Comparison | Float | Whitespace | …   
2. input stream a1a2a3...an이 주어졌을 때   
	![[Pasted image 20240408060804.png]] input stream a1a2a3...an이 Merged로 이루어진 Language라면 mIdx = i   
	a1a2...amIdx까지 partition, classify   
	a1a2...amIdx이 L(Keywords)에 속하거나 L(identifier)에 속한다면, 각 token의 priority를 만들어야 함.   
	위 pseudo code는 a1이 Merged에 속하지 않는 경우 등에 대한 error handling이 필요하다.   
3.  나머지 input stream에 대해 step2를 수행     
<br>
이를 쉽게 체크하기 위해 **[[Finite automata]]** 를 사용.
 ###  [[Subset (powerset) construction algorithm]] 이용.

